{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evEllLvI57D2"
      },
      "outputs": [],
      "source": [
        "mytext = \"\"\"Once upon a time, in a lush green meadow surrounded by tall, whispering trees, lived a little rabbit named Ruby.\n",
        "Ruby had soft, fluffy fur as white as snow and a pair of sparkling blue eyes that gleamed with curiosity. She loved exploring the meadow, hopping from one flower to another, and making new friends.\n",
        "But Ruby had one peculiar habit—she was always curious about what lay beyond the meadow.\n",
        "One sunny morning, as the bees buzzed and the birds sang cheerful songs, Ruby hopped over to her best friend, Timmy the turtle.\n",
        "\\\"Timmy,\\\" she said, twitching her little nose, \\\"have you ever wondered what lies beyond the big oak tree?\\\"\n",
        "Timmy, who loved nothing more than basking in the sun, yawned and said, \\\"Why leave the meadow? We have everything we need right here.\\\"\n",
        "\\\"But what if there's something amazing out there?\\\" Ruby insisted.\n",
        "Timmy chuckled. \\\"You’re too curious, Ruby! But if you go, be careful.\\\"\n",
        "Ruby’s excitement bubbled over. She packed a small bag of carrots and set off toward the big oak tree at the edge of the meadow. As she hopped past the tree, she found a narrow, winding path leading into a forest. The forest was filled with towering trees, colorful mushrooms, and the sweet scent of wildflowers.\n",
        "Suddenly, Ruby heard a faint sound. \\\"Tweet! Tweet!\\\" She looked up and saw a little bird with a bright red chest stuck in a bush.\n",
        "\\\"Are you okay?\\\" Ruby asked.\n",
        "\\\"I'm stuck,\\\" chirped the bird. \\\"Can you help me?\\\"\n",
        "Ruby carefully nibbled at the branches until the little bird was free. \\\"Thank you, kind rabbit,\\\" said the bird. \\\"I'm Robin. If you ever need help, just call me.\\\"\n",
        "Ruby smiled and continued her journey. Soon, she reached a sparkling stream with clear water and shiny pebbles. As she bent down to drink, she saw something glittering on the other side. Hopping across the smooth stones, she discovered a patch of golden flowers that shimmered in the sunlight.\n",
        "\\\"Wow,\\\" Ruby gasped. She picked a single golden flower to take back to the meadow.\n",
        "As she turned to leave, a deep voice rumbled, \\\"Who dares take my golden flower?\\\"\n",
        "Ruby froze. A large but kind-looking bear emerged from behind the trees.\n",
        "\\\"I'm sory,\\\" Ruby stammered. \\\"I didn’t mean to upset you. I’ve never seen anything so beautiful and wanted to share it with my friends.\\\"\n",
        "The bear smiled. \\\"You have a kind heart, little rabbit. Take the flower and share its beauty. But remember, the real treasure is the friends you make along the way.\\\"\n",
        "ked the bear and hopped back to the meadow, where she shared her golden flower and her adventure with Timmy and all the other animals. From that day on, Ruby realized that while her curiosity led her to new and exciting places.\n",
        "it was her kindness and friends that made her world truly magical.\n",
        "And so, Ruby continued to explore, always bringing back stories and treasures to share with her friends.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re                                                  # regular expression\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.]', '', text)           # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()               #Replaces all occurrences of one or more whitespace characters. strip() removes leading and trailing spaces.\n",
        "    return text\n",
        "\n",
        "mytext_cleaned = clean_text(mytext)\n",
        "mytext_cleaned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "PFOdi38GRixo",
        "outputId": "762a355f-77b3-4a0d-9e51-409f45531d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'once upon a time in a lush green meadow surrounded by tall whispering trees lived a little rabbit named ruby. ruby had soft fluffy fur as white as snow and a pair of sparkling blue eyes that gleamed with curiosity. she loved exploring the meadow hopping from one flower to another and making new friends. but ruby had one peculiar habitshe was always curious about what lay beyond the meadow. one sunny morning as the bees buzzed and the birds sang cheerful songs ruby hopped over to her best friend timmy the turtle. timmy she said twitching her little nose have you ever wondered what lies beyond the big oak tree timmy who loved nothing more than basking in the sun yawned and said why leave the meadow we have everything we need right here. but what if theres something amazing out there ruby insisted. timmy chuckled. youre too curious ruby but if you go be careful. rubys excitement bubbled over. she packed a small bag of carrots and set off toward the big oak tree at the edge of the meadow. as she hopped past the tree she found a narrow winding path leading into a forest. the forest was filled with towering trees colorful mushrooms and the sweet scent of wildflowers. suddenly ruby heard a faint sound. tweet tweet she looked up and saw a little bird with a bright red chest stuck in a bush. are you okay ruby asked. im stuck chirped the bird. can you help me ruby carefully nibbled at the branches until the little bird was free. thank you kind rabbit said the bird. im robin. if you ever need help just call me. ruby smiled and continued her journey. soon she reached a sparkling stream with clear water and shiny pebbles. as she bent down to drink she saw something glittering on the other side. hopping across the smooth stones she discovered a patch of golden flowers that shimmered in the sunlight. wow ruby gasped. she picked a single golden flower to take back to the meadow. as she turned to leave a deep voice rumbled who dares take my golden flower ruby froze. a large but kindlooking bear emerged from behind the trees. im sory ruby stammered. i didnt mean to upset you. ive never seen anything so beautiful and wanted to share it with my friends. the bear smiled. you have a kind heart little rabbit. take the flower and share its beauty. but remember the real treasure is the friends you make along the way. ked the bear and hopped back to the meadow where she shared her golden flower and her adventure with timmy and all the other animals. from that day on ruby realized that while her curiosity led her to new and exciting places. it was her kindness and friends that made her world truly magical. and so ruby continued to explore always bringing back stories and treasures to share with her friends.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n"
      ],
      "metadata": {
        "id": "YBmk3Gmv8ySN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let create an object of tokenizer\n",
        "tokenizer  = Tokenizer()"
      ],
      "metadata": {
        "id": "616Bjsw59B0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([mytext_cleaned])"
      ],
      "metadata": {
        "id": "cngqfxWK9olC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk7j3kIp9rlN",
        "outputId": "8b012d3a-9630-430f-b678-4f832a2bcf2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'ruby': 4,\n",
              " 'she': 5,\n",
              " 'to': 6,\n",
              " 'her': 7,\n",
              " 'you': 8,\n",
              " 'meadow': 9,\n",
              " 'with': 10,\n",
              " 'as': 11,\n",
              " 'little': 12,\n",
              " 'of': 13,\n",
              " 'that': 14,\n",
              " 'flower': 15,\n",
              " 'friends': 16,\n",
              " 'but': 17,\n",
              " 'timmy': 18,\n",
              " 'in': 19,\n",
              " 'was': 20,\n",
              " 'bird': 21,\n",
              " 'golden': 22,\n",
              " 'trees': 23,\n",
              " 'rabbit': 24,\n",
              " 'from': 25,\n",
              " 'one': 26,\n",
              " 'what': 27,\n",
              " 'hopped': 28,\n",
              " 'said': 29,\n",
              " 'have': 30,\n",
              " 'tree': 31,\n",
              " 'if': 32,\n",
              " 'im': 33,\n",
              " 'take': 34,\n",
              " 'back': 35,\n",
              " 'bear': 36,\n",
              " 'share': 37,\n",
              " 'had': 38,\n",
              " 'sparkling': 39,\n",
              " 'curiosity': 40,\n",
              " 'loved': 41,\n",
              " 'hopping': 42,\n",
              " 'new': 43,\n",
              " 'always': 44,\n",
              " 'curious': 45,\n",
              " 'beyond': 46,\n",
              " 'over': 47,\n",
              " 'ever': 48,\n",
              " 'big': 49,\n",
              " 'oak': 50,\n",
              " 'who': 51,\n",
              " 'leave': 52,\n",
              " 'we': 53,\n",
              " 'need': 54,\n",
              " 'something': 55,\n",
              " 'at': 56,\n",
              " 'forest': 57,\n",
              " 'tweet': 58,\n",
              " 'saw': 59,\n",
              " 'stuck': 60,\n",
              " 'help': 61,\n",
              " 'me': 62,\n",
              " 'kind': 63,\n",
              " 'smiled': 64,\n",
              " 'continued': 65,\n",
              " 'on': 66,\n",
              " 'other': 67,\n",
              " 'my': 68,\n",
              " 'so': 69,\n",
              " 'it': 70,\n",
              " 'once': 71,\n",
              " 'upon': 72,\n",
              " 'time': 73,\n",
              " 'lush': 74,\n",
              " 'green': 75,\n",
              " 'surrounded': 76,\n",
              " 'by': 77,\n",
              " 'tall': 78,\n",
              " 'whispering': 79,\n",
              " 'lived': 80,\n",
              " 'named': 81,\n",
              " 'soft': 82,\n",
              " 'fluffy': 83,\n",
              " 'fur': 84,\n",
              " 'white': 85,\n",
              " 'snow': 86,\n",
              " 'pair': 87,\n",
              " 'blue': 88,\n",
              " 'eyes': 89,\n",
              " 'gleamed': 90,\n",
              " 'exploring': 91,\n",
              " 'another': 92,\n",
              " 'making': 93,\n",
              " 'peculiar': 94,\n",
              " 'habitshe': 95,\n",
              " 'about': 96,\n",
              " 'lay': 97,\n",
              " 'sunny': 98,\n",
              " 'morning': 99,\n",
              " 'bees': 100,\n",
              " 'buzzed': 101,\n",
              " 'birds': 102,\n",
              " 'sang': 103,\n",
              " 'cheerful': 104,\n",
              " 'songs': 105,\n",
              " 'best': 106,\n",
              " 'friend': 107,\n",
              " 'turtle': 108,\n",
              " 'twitching': 109,\n",
              " 'nose': 110,\n",
              " 'wondered': 111,\n",
              " 'lies': 112,\n",
              " 'nothing': 113,\n",
              " 'more': 114,\n",
              " 'than': 115,\n",
              " 'basking': 116,\n",
              " 'sun': 117,\n",
              " 'yawned': 118,\n",
              " 'why': 119,\n",
              " 'everything': 120,\n",
              " 'right': 121,\n",
              " 'here': 122,\n",
              " 'theres': 123,\n",
              " 'amazing': 124,\n",
              " 'out': 125,\n",
              " 'there': 126,\n",
              " 'insisted': 127,\n",
              " 'chuckled': 128,\n",
              " 'youre': 129,\n",
              " 'too': 130,\n",
              " 'go': 131,\n",
              " 'be': 132,\n",
              " 'careful': 133,\n",
              " 'rubys': 134,\n",
              " 'excitement': 135,\n",
              " 'bubbled': 136,\n",
              " 'packed': 137,\n",
              " 'small': 138,\n",
              " 'bag': 139,\n",
              " 'carrots': 140,\n",
              " 'set': 141,\n",
              " 'off': 142,\n",
              " 'toward': 143,\n",
              " 'edge': 144,\n",
              " 'past': 145,\n",
              " 'found': 146,\n",
              " 'narrow': 147,\n",
              " 'winding': 148,\n",
              " 'path': 149,\n",
              " 'leading': 150,\n",
              " 'into': 151,\n",
              " 'filled': 152,\n",
              " 'towering': 153,\n",
              " 'colorful': 154,\n",
              " 'mushrooms': 155,\n",
              " 'sweet': 156,\n",
              " 'scent': 157,\n",
              " 'wildflowers': 158,\n",
              " 'suddenly': 159,\n",
              " 'heard': 160,\n",
              " 'faint': 161,\n",
              " 'sound': 162,\n",
              " 'looked': 163,\n",
              " 'up': 164,\n",
              " 'bright': 165,\n",
              " 'red': 166,\n",
              " 'chest': 167,\n",
              " 'bush': 168,\n",
              " 'are': 169,\n",
              " 'okay': 170,\n",
              " 'asked': 171,\n",
              " 'chirped': 172,\n",
              " 'can': 173,\n",
              " 'carefully': 174,\n",
              " 'nibbled': 175,\n",
              " 'branches': 176,\n",
              " 'until': 177,\n",
              " 'free': 178,\n",
              " 'thank': 179,\n",
              " 'robin': 180,\n",
              " 'just': 181,\n",
              " 'call': 182,\n",
              " 'journey': 183,\n",
              " 'soon': 184,\n",
              " 'reached': 185,\n",
              " 'stream': 186,\n",
              " 'clear': 187,\n",
              " 'water': 188,\n",
              " 'shiny': 189,\n",
              " 'pebbles': 190,\n",
              " 'bent': 191,\n",
              " 'down': 192,\n",
              " 'drink': 193,\n",
              " 'glittering': 194,\n",
              " 'side': 195,\n",
              " 'across': 196,\n",
              " 'smooth': 197,\n",
              " 'stones': 198,\n",
              " 'discovered': 199,\n",
              " 'patch': 200,\n",
              " 'flowers': 201,\n",
              " 'shimmered': 202,\n",
              " 'sunlight': 203,\n",
              " 'wow': 204,\n",
              " 'gasped': 205,\n",
              " 'picked': 206,\n",
              " 'single': 207,\n",
              " 'turned': 208,\n",
              " 'deep': 209,\n",
              " 'voice': 210,\n",
              " 'rumbled': 211,\n",
              " 'dares': 212,\n",
              " 'froze': 213,\n",
              " 'large': 214,\n",
              " 'kindlooking': 215,\n",
              " 'emerged': 216,\n",
              " 'behind': 217,\n",
              " 'sory': 218,\n",
              " 'stammered': 219,\n",
              " 'i': 220,\n",
              " 'didnt': 221,\n",
              " 'mean': 222,\n",
              " 'upset': 223,\n",
              " 'ive': 224,\n",
              " 'never': 225,\n",
              " 'seen': 226,\n",
              " 'anything': 227,\n",
              " 'beautiful': 228,\n",
              " 'wanted': 229,\n",
              " 'heart': 230,\n",
              " 'its': 231,\n",
              " 'beauty': 232,\n",
              " 'remember': 233,\n",
              " 'real': 234,\n",
              " 'treasure': 235,\n",
              " 'is': 236,\n",
              " 'make': 237,\n",
              " 'along': 238,\n",
              " 'way': 239,\n",
              " 'ked': 240,\n",
              " 'where': 241,\n",
              " 'shared': 242,\n",
              " 'adventure': 243,\n",
              " 'all': 244,\n",
              " 'animals': 245,\n",
              " 'day': 246,\n",
              " 'realized': 247,\n",
              " 'while': 248,\n",
              " 'led': 249,\n",
              " 'exciting': 250,\n",
              " 'places': 251,\n",
              " 'kindness': 252,\n",
              " 'made': 253,\n",
              " 'world': 254,\n",
              " 'truly': 255,\n",
              " 'magical': 256,\n",
              " 'explore': 257,\n",
              " 'bringing': 258,\n",
              " 'stories': 259,\n",
              " 'treasures': 260}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence = []\n",
        "# Split the text by newlines\n",
        "for sentence in mytext_cleaned.split('.'):\n",
        "    tokenized_sentance = tokenizer.texts_to_sequences([sentence])[0]            # Tokenize the sentence and convert to sequences\n",
        "    print(f'tokenized_sentence {tokenized_sentance} \\n')\n",
        "\n",
        "    for i in range(1, len(tokenized_sentance)):\n",
        "        partial_sequence = tokenized_sentance[:i+1]\n",
        "        input_sequence.append(partial_sequence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gE5PhIBg9ukH",
        "outputId": "99fedbba-3e4d-40fa-c7e6-dce82a80c529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenized_sentence [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80, 3, 12, 24, 81, 4] \n",
            "\n",
            "tokenized_sentence [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88, 89, 14, 90, 10, 40] \n",
            "\n",
            "tokenized_sentence [5, 41, 91, 1, 9, 42, 25, 26, 15, 6, 92, 2, 93, 43, 16] \n",
            "\n",
            "tokenized_sentence [17, 4, 38, 26, 94, 95, 20, 44, 45, 96, 27, 97, 46, 1, 9] \n",
            "\n",
            "tokenized_sentence [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4, 28, 47, 6, 7, 106, 107, 18, 1, 108] \n",
            "\n",
            "tokenized_sentence [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46, 1, 49, 50, 31, 18, 51, 41, 113, 114, 115, 116, 19, 1, 117, 118, 2, 29, 119, 52, 1, 9, 53, 30, 120, 53, 54, 121, 122] \n",
            "\n",
            "tokenized_sentence [17, 27, 32, 123, 55, 124, 125, 126, 4, 127] \n",
            "\n",
            "tokenized_sentence [18, 128] \n",
            "\n",
            "tokenized_sentence [129, 130, 45, 4, 17, 32, 8, 131, 132, 133] \n",
            "\n",
            "tokenized_sentence [134, 135, 136, 47] \n",
            "\n",
            "tokenized_sentence [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49, 50, 31, 56, 1, 144, 13, 1, 9] \n",
            "\n",
            "tokenized_sentence [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148, 149, 150, 151, 3, 57] \n",
            "\n",
            "tokenized_sentence [1, 57, 20, 152, 10, 153, 23, 154, 155, 2, 1, 156, 157, 13, 158] \n",
            "\n",
            "tokenized_sentence [159, 4, 160, 3, 161, 162] \n",
            "\n",
            "tokenized_sentence [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166, 167, 60, 19, 3, 168] \n",
            "\n",
            "tokenized_sentence [169, 8, 170, 4, 171] \n",
            "\n",
            "tokenized_sentence [33, 60, 172, 1, 21] \n",
            "\n",
            "tokenized_sentence [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177, 1, 12, 21, 20, 178] \n",
            "\n",
            "tokenized_sentence [179, 8, 63, 24, 29, 1, 21] \n",
            "\n",
            "tokenized_sentence [33, 180] \n",
            "\n",
            "tokenized_sentence [32, 8, 48, 54, 61, 181, 182, 62] \n",
            "\n",
            "tokenized_sentence [4, 64, 2, 65, 7, 183] \n",
            "\n",
            "tokenized_sentence [184, 5, 185, 3, 39, 186, 10, 187, 188, 2, 189, 190] \n",
            "\n",
            "tokenized_sentence [11, 5, 191, 192, 6, 193, 5, 59, 55, 194, 66, 1, 67, 195] \n",
            "\n",
            "tokenized_sentence [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201, 14, 202, 19, 1, 203] \n",
            "\n",
            "tokenized_sentence [204, 4, 205] \n",
            "\n",
            "tokenized_sentence [5, 206, 3, 207, 22, 15, 6, 34, 35, 6, 1, 9] \n",
            "\n",
            "tokenized_sentence [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34, 68, 22, 15, 4, 213] \n",
            "\n",
            "tokenized_sentence [3, 214, 17, 215, 36, 216, 25, 217, 1, 23] \n",
            "\n",
            "tokenized_sentence [33, 218, 4, 219] \n",
            "\n",
            "tokenized_sentence [220, 221, 222, 6, 223, 8] \n",
            "\n",
            "tokenized_sentence [224, 225, 226, 227, 69, 228, 2, 229, 6, 37, 70, 10, 68, 16] \n",
            "\n",
            "tokenized_sentence [1, 36, 64] \n",
            "\n",
            "tokenized_sentence [8, 30, 3, 63, 230, 12, 24] \n",
            "\n",
            "tokenized_sentence [34, 1, 15, 2, 37, 231, 232] \n",
            "\n",
            "tokenized_sentence [17, 233, 1, 234, 235, 236, 1, 16, 8, 237, 238, 1, 239] \n",
            "\n",
            "tokenized_sentence [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15, 2, 7, 243, 10, 18, 2, 244, 1, 67, 245] \n",
            "\n",
            "tokenized_sentence [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7, 6, 43, 2, 250, 251] \n",
            "\n",
            "tokenized_sentence [70, 20, 7, 252, 2, 16, 14, 253, 7, 254, 255, 256] \n",
            "\n",
            "tokenized_sentence [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260, 6, 37, 10, 7, 16] \n",
            "\n",
            "tokenized_sentence [] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YmF9o2_eDtST",
        "outputId": "f72b31b7-f163-416d-f7ab-6dad1cb7230c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[71, 72],\n",
              " [71, 72, 3],\n",
              " [71, 72, 3, 73],\n",
              " [71, 72, 3, 73, 19],\n",
              " [71, 72, 3, 73, 19, 3],\n",
              " [71, 72, 3, 73, 19, 3, 74],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80, 3],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80, 3, 12],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80, 3, 12, 24],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80, 3, 12, 24, 81],\n",
              " [71, 72, 3, 73, 19, 3, 74, 75, 9, 76, 77, 78, 79, 23, 80, 3, 12, 24, 81, 4],\n",
              " [4, 38],\n",
              " [4, 38, 82],\n",
              " [4, 38, 82, 83],\n",
              " [4, 38, 82, 83, 84],\n",
              " [4, 38, 82, 83, 84, 11],\n",
              " [4, 38, 82, 83, 84, 11, 85],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88, 89],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88, 89, 14],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88, 89, 14, 90],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88, 89, 14, 90, 10],\n",
              " [4, 38, 82, 83, 84, 11, 85, 11, 86, 2, 3, 87, 13, 39, 88, 89, 14, 90, 10, 40],\n",
              " [5, 41],\n",
              " [5, 41, 91],\n",
              " [5, 41, 91, 1],\n",
              " [5, 41, 91, 1, 9],\n",
              " [5, 41, 91, 1, 9, 42],\n",
              " [5, 41, 91, 1, 9, 42, 25],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15, 6],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15, 6, 92],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15, 6, 92, 2],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15, 6, 92, 2, 93],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15, 6, 92, 2, 93, 43],\n",
              " [5, 41, 91, 1, 9, 42, 25, 26, 15, 6, 92, 2, 93, 43, 16],\n",
              " [17, 4],\n",
              " [17, 4, 38],\n",
              " [17, 4, 38, 26],\n",
              " [17, 4, 38, 26, 94],\n",
              " [17, 4, 38, 26, 94, 95],\n",
              " [17, 4, 38, 26, 94, 95, 20],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45, 96],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45, 96, 27],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45, 96, 27, 97],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45, 96, 27, 97, 46],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45, 96, 27, 97, 46, 1],\n",
              " [17, 4, 38, 26, 94, 95, 20, 44, 45, 96, 27, 97, 46, 1, 9],\n",
              " [26, 98],\n",
              " [26, 98, 99],\n",
              " [26, 98, 99, 11],\n",
              " [26, 98, 99, 11, 1],\n",
              " [26, 98, 99, 11, 1, 100],\n",
              " [26, 98, 99, 11, 1, 100, 101],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4, 28],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4, 28, 47],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4, 28, 47, 6],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4, 28, 47, 6, 7],\n",
              " [26, 98, 99, 11, 1, 100, 101, 2, 1, 102, 103, 104, 105, 4, 28, 47, 6, 7, 106],\n",
              " [26,\n",
              "  98,\n",
              "  99,\n",
              "  11,\n",
              "  1,\n",
              "  100,\n",
              "  101,\n",
              "  2,\n",
              "  1,\n",
              "  102,\n",
              "  103,\n",
              "  104,\n",
              "  105,\n",
              "  4,\n",
              "  28,\n",
              "  47,\n",
              "  6,\n",
              "  7,\n",
              "  106,\n",
              "  107],\n",
              " [26,\n",
              "  98,\n",
              "  99,\n",
              "  11,\n",
              "  1,\n",
              "  100,\n",
              "  101,\n",
              "  2,\n",
              "  1,\n",
              "  102,\n",
              "  103,\n",
              "  104,\n",
              "  105,\n",
              "  4,\n",
              "  28,\n",
              "  47,\n",
              "  6,\n",
              "  7,\n",
              "  106,\n",
              "  107,\n",
              "  18],\n",
              " [26,\n",
              "  98,\n",
              "  99,\n",
              "  11,\n",
              "  1,\n",
              "  100,\n",
              "  101,\n",
              "  2,\n",
              "  1,\n",
              "  102,\n",
              "  103,\n",
              "  104,\n",
              "  105,\n",
              "  4,\n",
              "  28,\n",
              "  47,\n",
              "  6,\n",
              "  7,\n",
              "  106,\n",
              "  107,\n",
              "  18,\n",
              "  1],\n",
              " [26,\n",
              "  98,\n",
              "  99,\n",
              "  11,\n",
              "  1,\n",
              "  100,\n",
              "  101,\n",
              "  2,\n",
              "  1,\n",
              "  102,\n",
              "  103,\n",
              "  104,\n",
              "  105,\n",
              "  4,\n",
              "  28,\n",
              "  47,\n",
              "  6,\n",
              "  7,\n",
              "  106,\n",
              "  107,\n",
              "  18,\n",
              "  1,\n",
              "  108],\n",
              " [18, 5],\n",
              " [18, 5, 29],\n",
              " [18, 5, 29, 109],\n",
              " [18, 5, 29, 109, 7],\n",
              " [18, 5, 29, 109, 7, 12],\n",
              " [18, 5, 29, 109, 7, 12, 110],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46, 1],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46, 1, 49],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46, 1, 49, 50],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46, 1, 49, 50, 31],\n",
              " [18, 5, 29, 109, 7, 12, 110, 30, 8, 48, 111, 27, 112, 46, 1, 49, 50, 31, 18],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53,\n",
              "  30],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53,\n",
              "  30,\n",
              "  120],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53,\n",
              "  30,\n",
              "  120,\n",
              "  53],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53,\n",
              "  30,\n",
              "  120,\n",
              "  53,\n",
              "  54],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53,\n",
              "  30,\n",
              "  120,\n",
              "  53,\n",
              "  54,\n",
              "  121],\n",
              " [18,\n",
              "  5,\n",
              "  29,\n",
              "  109,\n",
              "  7,\n",
              "  12,\n",
              "  110,\n",
              "  30,\n",
              "  8,\n",
              "  48,\n",
              "  111,\n",
              "  27,\n",
              "  112,\n",
              "  46,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  18,\n",
              "  51,\n",
              "  41,\n",
              "  113,\n",
              "  114,\n",
              "  115,\n",
              "  116,\n",
              "  19,\n",
              "  1,\n",
              "  117,\n",
              "  118,\n",
              "  2,\n",
              "  29,\n",
              "  119,\n",
              "  52,\n",
              "  1,\n",
              "  9,\n",
              "  53,\n",
              "  30,\n",
              "  120,\n",
              "  53,\n",
              "  54,\n",
              "  121,\n",
              "  122],\n",
              " [17, 27],\n",
              " [17, 27, 32],\n",
              " [17, 27, 32, 123],\n",
              " [17, 27, 32, 123, 55],\n",
              " [17, 27, 32, 123, 55, 124],\n",
              " [17, 27, 32, 123, 55, 124, 125],\n",
              " [17, 27, 32, 123, 55, 124, 125, 126],\n",
              " [17, 27, 32, 123, 55, 124, 125, 126, 4],\n",
              " [17, 27, 32, 123, 55, 124, 125, 126, 4, 127],\n",
              " [18, 128],\n",
              " [129, 130],\n",
              " [129, 130, 45],\n",
              " [129, 130, 45, 4],\n",
              " [129, 130, 45, 4, 17],\n",
              " [129, 130, 45, 4, 17, 32],\n",
              " [129, 130, 45, 4, 17, 32, 8],\n",
              " [129, 130, 45, 4, 17, 32, 8, 131],\n",
              " [129, 130, 45, 4, 17, 32, 8, 131, 132],\n",
              " [129, 130, 45, 4, 17, 32, 8, 131, 132, 133],\n",
              " [134, 135],\n",
              " [134, 135, 136],\n",
              " [134, 135, 136, 47],\n",
              " [5, 137],\n",
              " [5, 137, 3],\n",
              " [5, 137, 3, 138],\n",
              " [5, 137, 3, 138, 139],\n",
              " [5, 137, 3, 138, 139, 13],\n",
              " [5, 137, 3, 138, 139, 13, 140],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49, 50],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49, 50, 31],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49, 50, 31, 56],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49, 50, 31, 56, 1],\n",
              " [5, 137, 3, 138, 139, 13, 140, 2, 141, 142, 143, 1, 49, 50, 31, 56, 1, 144],\n",
              " [5,\n",
              "  137,\n",
              "  3,\n",
              "  138,\n",
              "  139,\n",
              "  13,\n",
              "  140,\n",
              "  2,\n",
              "  141,\n",
              "  142,\n",
              "  143,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  56,\n",
              "  1,\n",
              "  144,\n",
              "  13],\n",
              " [5,\n",
              "  137,\n",
              "  3,\n",
              "  138,\n",
              "  139,\n",
              "  13,\n",
              "  140,\n",
              "  2,\n",
              "  141,\n",
              "  142,\n",
              "  143,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  56,\n",
              "  1,\n",
              "  144,\n",
              "  13,\n",
              "  1],\n",
              " [5,\n",
              "  137,\n",
              "  3,\n",
              "  138,\n",
              "  139,\n",
              "  13,\n",
              "  140,\n",
              "  2,\n",
              "  141,\n",
              "  142,\n",
              "  143,\n",
              "  1,\n",
              "  49,\n",
              "  50,\n",
              "  31,\n",
              "  56,\n",
              "  1,\n",
              "  144,\n",
              "  13,\n",
              "  1,\n",
              "  9],\n",
              " [11, 5],\n",
              " [11, 5, 28],\n",
              " [11, 5, 28, 145],\n",
              " [11, 5, 28, 145, 1],\n",
              " [11, 5, 28, 145, 1, 31],\n",
              " [11, 5, 28, 145, 1, 31, 5],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148, 149],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148, 149, 150],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148, 149, 150, 151],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148, 149, 150, 151, 3],\n",
              " [11, 5, 28, 145, 1, 31, 5, 146, 3, 147, 148, 149, 150, 151, 3, 57],\n",
              " [1, 57],\n",
              " [1, 57, 20],\n",
              " [1, 57, 20, 152],\n",
              " [1, 57, 20, 152, 10],\n",
              " [1, 57, 20, 152, 10, 153],\n",
              " [1, 57, 20, 152, 10, 153, 23],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155, 2],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155, 2, 1],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155, 2, 1, 156],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155, 2, 1, 156, 157],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155, 2, 1, 156, 157, 13],\n",
              " [1, 57, 20, 152, 10, 153, 23, 154, 155, 2, 1, 156, 157, 13, 158],\n",
              " [159, 4],\n",
              " [159, 4, 160],\n",
              " [159, 4, 160, 3],\n",
              " [159, 4, 160, 3, 161],\n",
              " [159, 4, 160, 3, 161, 162],\n",
              " [58, 58],\n",
              " [58, 58, 5],\n",
              " [58, 58, 5, 163],\n",
              " [58, 58, 5, 163, 164],\n",
              " [58, 58, 5, 163, 164, 2],\n",
              " [58, 58, 5, 163, 164, 2, 59],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166, 167],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166, 167, 60],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166, 167, 60, 19],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166, 167, 60, 19, 3],\n",
              " [58, 58, 5, 163, 164, 2, 59, 3, 12, 21, 10, 3, 165, 166, 167, 60, 19, 3, 168],\n",
              " [169, 8],\n",
              " [169, 8, 170],\n",
              " [169, 8, 170, 4],\n",
              " [169, 8, 170, 4, 171],\n",
              " [33, 60],\n",
              " [33, 60, 172],\n",
              " [33, 60, 172, 1],\n",
              " [33, 60, 172, 1, 21],\n",
              " [173, 8],\n",
              " [173, 8, 61],\n",
              " [173, 8, 61, 62],\n",
              " [173, 8, 61, 62, 4],\n",
              " [173, 8, 61, 62, 4, 174],\n",
              " [173, 8, 61, 62, 4, 174, 175],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177, 1],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177, 1, 12],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177, 1, 12, 21],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177, 1, 12, 21, 20],\n",
              " [173, 8, 61, 62, 4, 174, 175, 56, 1, 176, 177, 1, 12, 21, 20, 178],\n",
              " [179, 8],\n",
              " [179, 8, 63],\n",
              " [179, 8, 63, 24],\n",
              " [179, 8, 63, 24, 29],\n",
              " [179, 8, 63, 24, 29, 1],\n",
              " [179, 8, 63, 24, 29, 1, 21],\n",
              " [33, 180],\n",
              " [32, 8],\n",
              " [32, 8, 48],\n",
              " [32, 8, 48, 54],\n",
              " [32, 8, 48, 54, 61],\n",
              " [32, 8, 48, 54, 61, 181],\n",
              " [32, 8, 48, 54, 61, 181, 182],\n",
              " [32, 8, 48, 54, 61, 181, 182, 62],\n",
              " [4, 64],\n",
              " [4, 64, 2],\n",
              " [4, 64, 2, 65],\n",
              " [4, 64, 2, 65, 7],\n",
              " [4, 64, 2, 65, 7, 183],\n",
              " [184, 5],\n",
              " [184, 5, 185],\n",
              " [184, 5, 185, 3],\n",
              " [184, 5, 185, 3, 39],\n",
              " [184, 5, 185, 3, 39, 186],\n",
              " [184, 5, 185, 3, 39, 186, 10],\n",
              " [184, 5, 185, 3, 39, 186, 10, 187],\n",
              " [184, 5, 185, 3, 39, 186, 10, 187, 188],\n",
              " [184, 5, 185, 3, 39, 186, 10, 187, 188, 2],\n",
              " [184, 5, 185, 3, 39, 186, 10, 187, 188, 2, 189],\n",
              " [184, 5, 185, 3, 39, 186, 10, 187, 188, 2, 189, 190],\n",
              " [11, 5],\n",
              " [11, 5, 191],\n",
              " [11, 5, 191, 192],\n",
              " [11, 5, 191, 192, 6],\n",
              " [11, 5, 191, 192, 6, 193],\n",
              " [11, 5, 191, 192, 6, 193, 5],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59, 55],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59, 55, 194],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59, 55, 194, 66],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59, 55, 194, 66, 1],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59, 55, 194, 66, 1, 67],\n",
              " [11, 5, 191, 192, 6, 193, 5, 59, 55, 194, 66, 1, 67, 195],\n",
              " [42, 196],\n",
              " [42, 196, 1],\n",
              " [42, 196, 1, 197],\n",
              " [42, 196, 1, 197, 198],\n",
              " [42, 196, 1, 197, 198, 5],\n",
              " [42, 196, 1, 197, 198, 5, 199],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201, 14],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201, 14, 202],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201, 14, 202, 19],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201, 14, 202, 19, 1],\n",
              " [42, 196, 1, 197, 198, 5, 199, 3, 200, 13, 22, 201, 14, 202, 19, 1, 203],\n",
              " [204, 4],\n",
              " [204, 4, 205],\n",
              " [5, 206],\n",
              " [5, 206, 3],\n",
              " [5, 206, 3, 207],\n",
              " [5, 206, 3, 207, 22],\n",
              " [5, 206, 3, 207, 22, 15],\n",
              " [5, 206, 3, 207, 22, 15, 6],\n",
              " [5, 206, 3, 207, 22, 15, 6, 34],\n",
              " [5, 206, 3, 207, 22, 15, 6, 34, 35],\n",
              " [5, 206, 3, 207, 22, 15, 6, 34, 35, 6],\n",
              " [5, 206, 3, 207, 22, 15, 6, 34, 35, 6, 1],\n",
              " [5, 206, 3, 207, 22, 15, 6, 34, 35, 6, 1, 9],\n",
              " [11, 5],\n",
              " [11, 5, 208],\n",
              " [11, 5, 208, 6],\n",
              " [11, 5, 208, 6, 52],\n",
              " [11, 5, 208, 6, 52, 3],\n",
              " [11, 5, 208, 6, 52, 3, 209],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34, 68],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34, 68, 22],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34, 68, 22, 15],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34, 68, 22, 15, 4],\n",
              " [11, 5, 208, 6, 52, 3, 209, 210, 211, 51, 212, 34, 68, 22, 15, 4, 213],\n",
              " [3, 214],\n",
              " [3, 214, 17],\n",
              " [3, 214, 17, 215],\n",
              " [3, 214, 17, 215, 36],\n",
              " [3, 214, 17, 215, 36, 216],\n",
              " [3, 214, 17, 215, 36, 216, 25],\n",
              " [3, 214, 17, 215, 36, 216, 25, 217],\n",
              " [3, 214, 17, 215, 36, 216, 25, 217, 1],\n",
              " [3, 214, 17, 215, 36, 216, 25, 217, 1, 23],\n",
              " [33, 218],\n",
              " [33, 218, 4],\n",
              " [33, 218, 4, 219],\n",
              " [220, 221],\n",
              " [220, 221, 222],\n",
              " [220, 221, 222, 6],\n",
              " [220, 221, 222, 6, 223],\n",
              " [220, 221, 222, 6, 223, 8],\n",
              " [224, 225],\n",
              " [224, 225, 226],\n",
              " [224, 225, 226, 227],\n",
              " [224, 225, 226, 227, 69],\n",
              " [224, 225, 226, 227, 69, 228],\n",
              " [224, 225, 226, 227, 69, 228, 2],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229, 6],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229, 6, 37],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229, 6, 37, 70],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229, 6, 37, 70, 10],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229, 6, 37, 70, 10, 68],\n",
              " [224, 225, 226, 227, 69, 228, 2, 229, 6, 37, 70, 10, 68, 16],\n",
              " [1, 36],\n",
              " [1, 36, 64],\n",
              " [8, 30],\n",
              " [8, 30, 3],\n",
              " [8, 30, 3, 63],\n",
              " [8, 30, 3, 63, 230],\n",
              " [8, 30, 3, 63, 230, 12],\n",
              " [8, 30, 3, 63, 230, 12, 24],\n",
              " [34, 1],\n",
              " [34, 1, 15],\n",
              " [34, 1, 15, 2],\n",
              " [34, 1, 15, 2, 37],\n",
              " [34, 1, 15, 2, 37, 231],\n",
              " [34, 1, 15, 2, 37, 231, 232],\n",
              " [17, 233],\n",
              " [17, 233, 1],\n",
              " [17, 233, 1, 234],\n",
              " [17, 233, 1, 234, 235],\n",
              " [17, 233, 1, 234, 235, 236],\n",
              " [17, 233, 1, 234, 235, 236, 1],\n",
              " [17, 233, 1, 234, 235, 236, 1, 16],\n",
              " [17, 233, 1, 234, 235, 236, 1, 16, 8],\n",
              " [17, 233, 1, 234, 235, 236, 1, 16, 8, 237],\n",
              " [17, 233, 1, 234, 235, 236, 1, 16, 8, 237, 238],\n",
              " [17, 233, 1, 234, 235, 236, 1, 16, 8, 237, 238, 1],\n",
              " [17, 233, 1, 234, 235, 236, 1, 16, 8, 237, 238, 1, 239],\n",
              " [240, 1],\n",
              " [240, 1, 36],\n",
              " [240, 1, 36, 2],\n",
              " [240, 1, 36, 2, 28],\n",
              " [240, 1, 36, 2, 28, 35],\n",
              " [240, 1, 36, 2, 28, 35, 6],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15, 2],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15, 2, 7],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15, 2, 7, 243],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15, 2, 7, 243, 10],\n",
              " [240, 1, 36, 2, 28, 35, 6, 1, 9, 241, 5, 242, 7, 22, 15, 2, 7, 243, 10, 18],\n",
              " [240,\n",
              "  1,\n",
              "  36,\n",
              "  2,\n",
              "  28,\n",
              "  35,\n",
              "  6,\n",
              "  1,\n",
              "  9,\n",
              "  241,\n",
              "  5,\n",
              "  242,\n",
              "  7,\n",
              "  22,\n",
              "  15,\n",
              "  2,\n",
              "  7,\n",
              "  243,\n",
              "  10,\n",
              "  18,\n",
              "  2],\n",
              " [240,\n",
              "  1,\n",
              "  36,\n",
              "  2,\n",
              "  28,\n",
              "  35,\n",
              "  6,\n",
              "  1,\n",
              "  9,\n",
              "  241,\n",
              "  5,\n",
              "  242,\n",
              "  7,\n",
              "  22,\n",
              "  15,\n",
              "  2,\n",
              "  7,\n",
              "  243,\n",
              "  10,\n",
              "  18,\n",
              "  2,\n",
              "  244],\n",
              " [240,\n",
              "  1,\n",
              "  36,\n",
              "  2,\n",
              "  28,\n",
              "  35,\n",
              "  6,\n",
              "  1,\n",
              "  9,\n",
              "  241,\n",
              "  5,\n",
              "  242,\n",
              "  7,\n",
              "  22,\n",
              "  15,\n",
              "  2,\n",
              "  7,\n",
              "  243,\n",
              "  10,\n",
              "  18,\n",
              "  2,\n",
              "  244,\n",
              "  1],\n",
              " [240,\n",
              "  1,\n",
              "  36,\n",
              "  2,\n",
              "  28,\n",
              "  35,\n",
              "  6,\n",
              "  1,\n",
              "  9,\n",
              "  241,\n",
              "  5,\n",
              "  242,\n",
              "  7,\n",
              "  22,\n",
              "  15,\n",
              "  2,\n",
              "  7,\n",
              "  243,\n",
              "  10,\n",
              "  18,\n",
              "  2,\n",
              "  244,\n",
              "  1,\n",
              "  67],\n",
              " [240,\n",
              "  1,\n",
              "  36,\n",
              "  2,\n",
              "  28,\n",
              "  35,\n",
              "  6,\n",
              "  1,\n",
              "  9,\n",
              "  241,\n",
              "  5,\n",
              "  242,\n",
              "  7,\n",
              "  22,\n",
              "  15,\n",
              "  2,\n",
              "  7,\n",
              "  243,\n",
              "  10,\n",
              "  18,\n",
              "  2,\n",
              "  244,\n",
              "  1,\n",
              "  67,\n",
              "  245],\n",
              " [25, 14],\n",
              " [25, 14, 246],\n",
              " [25, 14, 246, 66],\n",
              " [25, 14, 246, 66, 4],\n",
              " [25, 14, 246, 66, 4, 247],\n",
              " [25, 14, 246, 66, 4, 247, 14],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7, 6],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7, 6, 43],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7, 6, 43, 2],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7, 6, 43, 2, 250],\n",
              " [25, 14, 246, 66, 4, 247, 14, 248, 7, 40, 249, 7, 6, 43, 2, 250, 251],\n",
              " [70, 20],\n",
              " [70, 20, 7],\n",
              " [70, 20, 7, 252],\n",
              " [70, 20, 7, 252, 2],\n",
              " [70, 20, 7, 252, 2, 16],\n",
              " [70, 20, 7, 252, 2, 16, 14],\n",
              " [70, 20, 7, 252, 2, 16, 14, 253],\n",
              " [70, 20, 7, 252, 2, 16, 14, 253, 7],\n",
              " [70, 20, 7, 252, 2, 16, 14, 253, 7, 254],\n",
              " [70, 20, 7, 252, 2, 16, 14, 253, 7, 254, 255],\n",
              " [70, 20, 7, 252, 2, 16, 14, 253, 7, 254, 255, 256],\n",
              " [2, 69],\n",
              " [2, 69, 4],\n",
              " [2, 69, 4, 65],\n",
              " [2, 69, 4, 65, 6],\n",
              " [2, 69, 4, 65, 6, 257],\n",
              " [2, 69, 4, 65, 6, 257, 44],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260, 6],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260, 6, 37],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260, 6, 37, 10],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260, 6, 37, 10, 7],\n",
              " [2, 69, 4, 65, 6, 257, 44, 258, 35, 259, 2, 260, 6, 37, 10, 7, 16]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need padding zero as for supervised learning the label data size should be equal\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_sequence_length = max([len(seq) for seq in input_sequence])\n",
        "input_sequence_padded = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='pre')     #In deep learning, sequences of different lengths need to be transformed into sequences of the same length before they can be fed into models.\n"
      ],
      "metadata": {
        "id": "qscQwx3Eill1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence_padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efr9nnrhmwsp",
        "outputId": "c9ac1bb5-77c5-44d6-ecd5-e7fdbb3cf077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  0, 71, 72],\n",
              "       [ 0,  0,  0, ..., 71, 72,  3],\n",
              "       [ 0,  0,  0, ..., 72,  3, 73],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ...,  6, 37, 10],\n",
              "       [ 0,  0,  0, ..., 37, 10,  7],\n",
              "       [ 0,  0,  0, ..., 10,  7, 16]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to splite vector into dependent and independt vectors\n",
        "X = input_sequence_padded[:, :-1]\n",
        "y = input_sequence_padded[:, -1]"
      ],
      "metadata": {
        "id": "BIicqvsCmyfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqIZx5yqnaSA",
        "outputId": "a519d777-81dd-4aa5-8a6d-0dfa087d2b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(457, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)\n",
        "len(tokenizer.word_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otbBlwCQnipG",
        "outputId": "e6691c19-ab19-496a-f01d-4443d4a00450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(457,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now this is the mulicalss classification problem\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "y_encoded = to_categorical(y, num_classes=len(tokenizer.word_index)+1)    # we added 1 because one hot encoding start from 0\n",
        "y_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlc8lFylnmqo",
        "outputId": "7a787bf1-8e40-471c-e7e5-c9970500a05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded.shape\n",
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNZyoqqzszNf",
        "outputId": "a634828e-bb8a-41c5-a35e-8ace20763cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(261, 100, input_length=41))  # output dim = 100 for word embedding , 261 input dim\n",
        "model.add(LSTM(150))                             # 150 units in LSTM layer\n",
        "model.add(Dense(261, activation='softmax'))      # output layer for 261 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',                           # Using the Adam optimizer\n",
        "    loss='categorical_crossentropy',            # Loss function for multi-class classification\n",
        "    metrics=['accuracy']                        # Track accuracy during training\n",
        ")\n",
        "\n",
        "# Build the model by passing dummy data (of shape (batch_size, input_length))\n",
        "model.build(input_shape=(None, 41))  # None for batch size, 41 for input length\n",
        "\n",
        "# Now you can print the summary of the model\n",
        "model.summary()                                                        #After defining your model but before training to ensure it is configured correctly.\n",
        "\n",
        "#total params represent the total number of weights and baises\n",
        "# 41 represent number of total words\n",
        "# total timesteps will be 41\n",
        "# the output will be 261 amoung which softmax will consider highest probability output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "2kDdxp1qDHkG",
        "outputId": "64a04149-5468-456f-ef0c-75c60af5e792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │          \u001b[38;5;34m26,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │         \u001b[38;5;34m150,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m261\u001b[0m)                 │          \u001b[38;5;34m39,411\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">26,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">261</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">39,411</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m216,111\u001b[0m (844.18 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,111</span> (844.18 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m216,111\u001b[0m (844.18 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">216,111</span> (844.18 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to train our model\n",
        "\n",
        "model.fit(X, y_encoded, epochs=100)\n",
        "model.save('Next_word predication LSTM.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA7sMGNdHwbC",
        "outputId": "55030a9c-7b91-4c28-dec6-c3391b3c6499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.0255 - loss: 5.5532\n",
            "Epoch 2/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.0424 - loss: 5.2520\n",
            "Epoch 3/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0504 - loss: 5.1547\n",
            "Epoch 4/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0634 - loss: 5.0499\n",
            "Epoch 5/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0454 - loss: 5.1312\n",
            "Epoch 6/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0704 - loss: 4.9844\n",
            "Epoch 7/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0679 - loss: 4.9193\n",
            "Epoch 8/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0646 - loss: 4.8709\n",
            "Epoch 9/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0652 - loss: 4.8919\n",
            "Epoch 10/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0869 - loss: 4.7562\n",
            "Epoch 11/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0755 - loss: 4.7032\n",
            "Epoch 12/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0757 - loss: 4.5733\n",
            "Epoch 13/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0906 - loss: 4.4958\n",
            "Epoch 14/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0967 - loss: 4.3791\n",
            "Epoch 15/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1303 - loss: 4.1684\n",
            "Epoch 16/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1469 - loss: 4.0429\n",
            "Epoch 17/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1807 - loss: 3.8246\n",
            "Epoch 18/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2045 - loss: 3.7036\n",
            "Epoch 19/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1995 - loss: 3.5287\n",
            "Epoch 20/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2533 - loss: 3.3634\n",
            "Epoch 21/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2958 - loss: 3.2315\n",
            "Epoch 22/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2952 - loss: 3.0599\n",
            "Epoch 23/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3519 - loss: 2.8846\n",
            "Epoch 24/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3807 - loss: 2.8136\n",
            "Epoch 25/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4635 - loss: 2.6223\n",
            "Epoch 26/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4713 - loss: 2.4984\n",
            "Epoch 27/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4749 - loss: 2.3339\n",
            "Epoch 28/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5460 - loss: 2.2091\n",
            "Epoch 29/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5716 - loss: 2.1380\n",
            "Epoch 30/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5993 - loss: 1.9802\n",
            "Epoch 31/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6387 - loss: 1.9323\n",
            "Epoch 32/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7094 - loss: 1.7379\n",
            "Epoch 33/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7136 - loss: 1.7112\n",
            "Epoch 34/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7117 - loss: 1.6485\n",
            "Epoch 35/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7719 - loss: 1.5383\n",
            "Epoch 36/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7841 - loss: 1.4854\n",
            "Epoch 37/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7771 - loss: 1.4239\n",
            "Epoch 38/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8309 - loss: 1.2566\n",
            "Epoch 39/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8770 - loss: 1.2213\n",
            "Epoch 40/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8619 - loss: 1.1622\n",
            "Epoch 41/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9021 - loss: 1.0635\n",
            "Epoch 42/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8756 - loss: 1.0799\n",
            "Epoch 43/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9015 - loss: 0.9776\n",
            "Epoch 44/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9120 - loss: 0.9539\n",
            "Epoch 45/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9359 - loss: 0.8555\n",
            "Epoch 46/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9407 - loss: 0.8260\n",
            "Epoch 47/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9359 - loss: 0.7711\n",
            "Epoch 48/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9435 - loss: 0.7747\n",
            "Epoch 49/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.7019\n",
            "Epoch 50/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9453 - loss: 0.7061\n",
            "Epoch 51/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9625 - loss: 0.6416\n",
            "Epoch 52/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9670 - loss: 0.5979\n",
            "Epoch 53/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9653 - loss: 0.5833\n",
            "Epoch 54/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9641 - loss: 0.5729\n",
            "Epoch 55/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9653 - loss: 0.5549\n",
            "Epoch 56/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9685 - loss: 0.4919\n",
            "Epoch 57/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9806 - loss: 0.4791\n",
            "Epoch 58/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9648 - loss: 0.5051\n",
            "Epoch 59/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9730 - loss: 0.4623\n",
            "Epoch 60/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9658 - loss: 0.4339\n",
            "Epoch 61/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9730 - loss: 0.4108\n",
            "Epoch 62/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9735 - loss: 0.3933\n",
            "Epoch 63/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9798 - loss: 0.3530\n",
            "Epoch 64/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9657 - loss: 0.3676\n",
            "Epoch 65/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9629 - loss: 0.3515\n",
            "Epoch 66/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9708 - loss: 0.3351\n",
            "Epoch 67/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9745 - loss: 0.3132\n",
            "Epoch 68/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9637 - loss: 0.3229\n",
            "Epoch 69/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9805 - loss: 0.2934\n",
            "Epoch 70/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9657 - loss: 0.2902\n",
            "Epoch 71/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9660 - loss: 0.2871\n",
            "Epoch 72/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9549 - loss: 0.2779\n",
            "Epoch 73/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9808 - loss: 0.2641\n",
            "Epoch 74/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.2418\n",
            "Epoch 75/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9822 - loss: 0.2369\n",
            "Epoch 76/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9722 - loss: 0.2423\n",
            "Epoch 77/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9726 - loss: 0.2259\n",
            "Epoch 78/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.2159\n",
            "Epoch 79/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9682 - loss: 0.2229\n",
            "Epoch 80/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9827 - loss: 0.1971\n",
            "Epoch 81/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.1971\n",
            "Epoch 82/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9709 - loss: 0.1918\n",
            "Epoch 83/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9595 - loss: 0.2072\n",
            "Epoch 84/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.1786\n",
            "Epoch 85/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9766 - loss: 0.1738\n",
            "Epoch 86/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9727 - loss: 0.1866\n",
            "Epoch 87/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9705 - loss: 0.1677\n",
            "Epoch 88/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.1695\n",
            "Epoch 89/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9752 - loss: 0.1623\n",
            "Epoch 90/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9817 - loss: 0.1595\n",
            "Epoch 91/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9774 - loss: 0.1486\n",
            "Epoch 92/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9609 - loss: 0.1639\n",
            "Epoch 93/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9700 - loss: 0.1523\n",
            "Epoch 94/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9632 - loss: 0.1653\n",
            "Epoch 95/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9767 - loss: 0.1486\n",
            "Epoch 96/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9715 - loss: 0.1428\n",
            "Epoch 97/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9647 - loss: 0.1397\n",
            "Epoch 98/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9685 - loss: 0.1377\n",
            "Epoch 99/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.1057\n",
            "Epoch 100/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9670 - loss: 0.1434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model(\"Next_word predication LSTM.h5\")  # For HDF5 format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6sr3aYTJeN4",
        "outputId": "77a2d935-0f8e-4c40-b61b-d9d6143a1e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "input_text = \"Once\"\n",
        "\n",
        "for i in range(10):\n",
        "  text_encoded = tokenizer.fit_on_texts([input_text])\n",
        "  input_sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
        "  input_sequence = pad_sequences([input_sequence], maxlen=41, padding='pre')\n",
        "  # print(input_sequence)\n",
        "\n",
        "  #now pass to model\n",
        "  prdct = np.argmax(model.predict(input_sequence))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == prdct:\n",
        "      input_text = input_text + \" \" + word\n",
        "      print(input_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "LT9XP-GdNwLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EM-96ObIOy6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}